{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NPZ Dataset to TFRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['k49-train-imgs.npz', 'k49-train-labels.npz', 'k49-test-imgs.npz', 'k49-test-labels.npz']\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/'\n",
    "\n",
    "datafiles = [file.split(\"/\")[-1] for file in os.environ['DATA_SOURCE'].split()]\n",
    "classmaps = [file for file in datafiles if file.split('.')[-1] == 'csv']\n",
    "datafiles = [file for file in datafiles if file.split('.')[-1] == 'npz']\n",
    "\n",
    "print(datafiles)\n",
    "\n",
    "def load(f):\n",
    "    return np.load(f)['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "\n",
    "def get_ds(x, y):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    ds = ds.shuffle(x.shape[0])\n",
    "    ds = ds.batch(batch_size)\n",
    "    it = ds.make_initializable_iterator()\n",
    "    (images, labels) = it.get_next()\n",
    "    images = tf.image.convert_image_dtype(images, tf.float32)\n",
    "    if len(images.get_shape()) <= 3:\n",
    "        images = tf.expand_dims(images, 3)\n",
    "    return it, (images, labels)\n",
    "\n",
    "def get_k49():\n",
    "    x_train, y_train, x_test, y_test = \\\n",
    "        [load(data_path + file_name) for file_name in datafiles]\n",
    "    return get_ds(x_train, y_train), get_ds(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Block used to build NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x, in_f, momentum, activation, scope, training):\n",
    "    \"\"\"\n",
    "    Non-bottleneck-2D shape detector, not sure if it works at signal\n",
    "    :param x: input model\n",
    "    :param in_f: number of input filters\n",
    "    :param momentum: momentum for batch normalization (should be near 0.99)\n",
    "    :param activation: activation function\n",
    "    :param scope: name of scope, please use scope_describer\n",
    "    :param training: determine if its training operation for BN\n",
    "    :return: output model\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        model = tf.layers.conv2d(x, in_f, 3, 1, 'SAME')\n",
    "        model = tf.layers.batch_normalization(model, momentum=momentum, training=training)\n",
    "        model = activation(model)\n",
    "\n",
    "        model = tf.layers.conv2d(model, in_f, 3, 1, 'SAME')\n",
    "        model = tf.layers.batch_normalization(model, momentum=momentum, training=training)\n",
    "        model = activation(model)\n",
    "\n",
    "        # model = tf.layers.conv1d(model, in_f, 3, 1, 'SAME')\n",
    "        # model = tf.layers.batch_normalization(model, momentum=momentum, training=training)\n",
    "        # model = activation(model)\n",
    "        #\n",
    "        # model = tf.layers.conv1d(model, in_f, 3, 1, 'SAME')\n",
    "        # model = tf.layers.batch_normalization(model, momentum=momentum, training=training)\n",
    "        # model = activation(model)\n",
    "\n",
    "        model = model + x\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def down_sample(x, in_f, out_f, momentum, activation, training, scope, conv, max_pooling):\n",
    "    \"\"\"\n",
    "    Down sampling convolution 2D block with side pooling\n",
    "    BN + concat + activation\n",
    "    :param max_pooling: max_pooling layer function\n",
    "    :param conv: conv layer function\n",
    "    :param x: input model\n",
    "    :param in_f: number of input filters\n",
    "    :param out_f: number of output filters\n",
    "    :param momentum: momentum for batch normalization (should be near 0.99)\n",
    "    :param activation: activation function\n",
    "    :param scope: name of scope, please use scope_describer\n",
    "    :param training: determine if its training operation for BN\n",
    "    :return: output model\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n",
    "        model_conv = conv(x, out_f - in_f, 3, 2, 'SAME')\n",
    "        # tf.contrib.layers.summaries.summarize_weights()\n",
    "        # tf.contrib.layers.summaries.summarize_biases()\n",
    "        # tf.summary.histogram(\"down_sample/\" + scope, model_conv)\n",
    "\n",
    "        model_pool = max_pooling(x, 3, 2, 'SAME')\n",
    "\n",
    "        model = tf.concat([model_conv, model_pool], axis=-1)\n",
    "\n",
    "        model = tf.layers.batch_normalization(model, momentum=momentum, training=training)\n",
    "        model = activation(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def down_sample_2d(x, in_f, out_f, momentum, activation, training, scope):\n",
    "    return down_sample(x, in_f, out_f, momentum, activation, training, scope,\n",
    "                       tf.layers.conv2d, tf.layers.max_pooling2d)\n",
    "\n",
    "\n",
    "def down_sample_1d(x, in_f, out_f, momentum, activation, training, scope):\n",
    "    return down_sample(x, in_f, out_f, momentum, activation, training, scope,\n",
    "                       tf.layers.conv1d, tf.layers.max_pooling1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ema_getter(ema, training):\n",
    "    def ema_getter(getter, name, *args, **kwargs):\n",
    "        var = getter(name, *args, **kwargs)\n",
    "        if ema is not None and not training:\n",
    "            ema_var = ema.average(var)\n",
    "            return ema_var if ema_var else var\n",
    "        return var\n",
    "\n",
    "    return ema_getter\n",
    "\n",
    "\n",
    "def model(in_image, training, scope, ema, first_filter=64):\n",
    "    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE,\n",
    "                           custom_getter=get_ema_getter(ema, training),\n",
    "                           regularizer=tf.contrib.layers.l2_regularizer(1e-3)\n",
    "                           ):\n",
    "        model = in_image\n",
    "        model = down_sample_2d(model, model.shape[-1], first_filter, momentum=0.999, activation=tf.nn.relu,\n",
    "                               training=training,\n",
    "                               scope='conv_1')\n",
    "        model = residual(model, model.shape[-1], momentum=0.99, activation=tf.nn.relu, training=training,\n",
    "                         scope='residual_1_1')\n",
    "\n",
    "        model = residual(model, model.shape[-1], momentum=0.99, activation=tf.nn.relu, training=training,\n",
    "                         scope='residual_1_2')\n",
    "\n",
    "        model = down_sample_2d(model, model.shape[-1], 2 * first_filter, momentum=0.999, activation=tf.nn.relu,\n",
    "                               training=training,\n",
    "                               scope='conv_2')\n",
    "        model = residual(model, model.shape[-1], momentum=0.99, activation=tf.nn.relu, training=training,\n",
    "                         scope='residual_2_1')\n",
    "\n",
    "        model = residual(model, model.shape[-1], momentum=0.99, activation=tf.nn.relu, training=training,\n",
    "                         scope='residual_2_2')\n",
    "\n",
    "        model = down_sample_2d(model, model.shape[-1], 4 * first_filter, momentum=0.999, activation=tf.nn.relu,\n",
    "                               training=training,\n",
    "                               scope='conv_3')\n",
    "\n",
    "        model = residual(model, model.shape[-1], momentum=0.99, activation=tf.nn.relu, training=training,\n",
    "                         scope='residual_3_1')\n",
    "\n",
    "        model = residual(model, model.shape[-1], momentum=0.99, activation=tf.nn.relu, training=training,\n",
    "                         scope='residual_3_2')\n",
    "\n",
    "        model = tf.layers.flatten(model)\n",
    "\n",
    "        model = tf.layers.dense(model, 200, activation=tf.nn.relu)\n",
    "        model = tf.layers.dense(model, 2)\n",
    "\n",
    "        model = tf.reshape(model, [-1, 1, 2])\n",
    "\n",
    "        return model\n",
    "    \n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "ema = tf.train.ExponentialMovingAverage(0.999, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(sess, iterator, train_op, loss, acc, num_epochs):\n",
    "    for i in range(num_epochs):\n",
    "        sess.run(iterator.initializer)\n",
    "        part_metrics = [[], []]\n",
    "        while True:\n",
    "            try:\n",
    "                _, l, acc = sess.run([train_op, loss, acc])\n",
    "                part_metrics[0].append(l)\n",
    "                part_metrics[1].append(acc)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "    print(np.mean(np.array(part_metrics), axis=0))\n",
    "    # print('Total epoch {0} loss: {1}'.format(i, sum(total_loss) / len(total_loss)))\n",
    "    \n",
    "def test(sess, test_output, acc):\n",
    "    sess.run(iterator.initializer)\n",
    "    metrics = [[]]\n",
    "    while True:\n",
    "        try:\n",
    "            acc = sess.run([acc,])\n",
    "            part_metrics[0].append(acc)\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    print(np.mean(np.array(part_metrics), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n",
      "(?, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "# IN PROGRESS\n",
    "(train_iterator, (x_train, gt_train)), (test_iterator, (x_test, gt_test)) = get_k49()\n",
    "\n",
    "print(gt_train.shape)\n",
    "num_classes = 49\n",
    "\n",
    "train_output = model(x_train, True, \"model\", ema, 32)\n",
    "test_output = model(x_test, False, \"model\", ema, 32)\n",
    "\n",
    "print(train_output.shape)\n",
    "\n",
    "gt_train_one = tf.one_hot(gt_train, num_classes)\n",
    "gt_test_one = tf.one_hot(gt_test, num_classes)\n",
    "\n",
    "test_label = tf.argmax(test_output, -1)\n",
    "train_label = tf.argmax(train_output, -1)\n",
    "\n",
    "test_acc, test_acc_op = tf.metrics.accuracy(labels=test_label,\n",
    "                                                predictions=tf.argmax(test_output, -1))\n",
    "train_acc, train_acc_op = tf.metrics.accuracy(labels=train_label,\n",
    "                                                predictions=tf.argmax(train_output, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
